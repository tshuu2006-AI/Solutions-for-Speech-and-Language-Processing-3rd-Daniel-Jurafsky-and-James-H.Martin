\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{}
\geometry{a4paper, margin=1in}

\title{Lab 6: Vector Semantics and Embeddings}
\author{Natural Language Processing}
\date{}

\begin{document}

\maketitle

\section*{Informations}
The chapter 6 in the \textit{Speech and Language Processing} (Jurafsky \& Martin) doesn't have its own exercises, so these exercises are created by ChatGPT.
\section*{Overviews}
\begin{itemize}[label=--]
  \item \textbf{Subject}: Natural language processing
  \item \textbf{Topic}: Vector Semantics, TF-IDF, PPMI, Word Embeddings
  \item \textbf{Durations}: 2 lectures x 90 minutes
  \item \textbf{Tools}: Python (NumPy, scikit-learn), Jupyter Notebook or Colab
\end{itemize}

\section*{Objectives}
After finishing this assignments, students can:
\begin{itemize}
    \item Understand and use cosine similarity, PMI, PPMI, TF-IDF
    \item Build co-occurrence matrix
    \item Use TF-IDF and PPMI to represent words and documents
    \item train word embeddings for analogy calculate similarity
    \item Compare TF-IDF and PPMI in the classification problems
\end{itemize}

\section*{Pre-lab Reading}
Chapter 6 in \textit{Speech and Language Processing} (Jurafsky \& Martin)

\begin{itemize}
    \item 6.1 Vector Semantics
    \item 6.3 TF-IDF
    \item 6.6 PMI and PPMI
    \item 6.7 Embeddings via Matrix Factorization
\end{itemize}

\section*{Exercises}

\subsection*{I. Theory}

\paragraph{Ex 1. Compute PMI/PPMI.}
for 3 documents:

\begin{quote}
Doc1: \texttt{"dogs bark loudly"} \\
Doc2: \texttt{"cats meow softly"} \\
Doc3: \texttt{"dogs and cats play"}
\end{quote}

\begin{itemize}
    \item Build a co-occurences matrix with context window size = 1 (remoce stopwords)
    \item Compute PMI and PPMI for pairs of words: ("dogs", "bark"), ("cats", "meow"), ("dogs", "cats")
\end{itemize}

\paragraph{Ex 2. Cosine similarity.}
Given 3 word vectors (normalized):

\[
\text{king} = [0.7, 0.1, 0.3],\quad
\text{queen} = [0.69, 0.12, 0.31],\quad
\text{man} = [0.5, 0.09, 0.4]
\]

\begin{itemize}
    \item Compute cosine similarity between: (king, queen) and (king, man)
    \item Analyze why king is nearer queen than man in the vector space
\end{itemize}

\paragraph{Ex 3. Answers the questions:}
\begin{enumerate}
    \item what are the mathematical differences between PPMI and TF-IDF mathematically and their representing objectives?
    \item when does PPMI cannot represent well?
    \item Advantages and disadvantages of TF-IDF comparing to word embeddings?
\end{enumerate}

\subsection*{II. Practice}

\paragraph{Ex 4. TF-IDF + Cosine Similarity}
\begin{itemize}
    \item Pick 5 documents
    \item Compute TF-IDF vector for each documents
    \item Find the most similar pair of documents (cosine similarity)
    \item Print 3 words has the highest TF-IDF for each documents
\end{itemize}

\paragraph{Ex 5. PPMI Matrix + Visualization}
\begin{itemize}
    \item Create a co-occurances matrix for small documents
    \item Compute PPMI matrix
    \item Use SVD to reduce the dimension to 2 dimension
    \item Plot the words to the 2D spaces (matplotlib)
\end{itemize}

\subsection*{III. Advanced â€“ Pratical applications}

\paragraph{Ex 6. Word Analogies with GloVe}
\begin{itemize}
    \item download GloVe (glove.6B.100d.txt)
    \item Compute:
    \[
    \text{king} - \text{man} + \text{woman} \approx \ ? \\
    \]
    \[
    \text{paris} - \text{france} + \text{italy} \approx \ ?
    \]
    \item Print top 5 similar words
\end{itemize}

\paragraph{Ex 7. Classify with TF-IDF vs PPMI}
\begin{itemize}
    \item Get binary documents (ex: positive/negative review)
    \item Represent each documents by:
    \begin{itemize}
        \item a) TF-IDF vector
        \item b) PPMI vector (the mean of PPMI for each word)
    \end{itemize}
    \item Use Logistic Regression to classify
    \item Comparing Accuracy and F1-score
\end{itemize}
\end{document}
