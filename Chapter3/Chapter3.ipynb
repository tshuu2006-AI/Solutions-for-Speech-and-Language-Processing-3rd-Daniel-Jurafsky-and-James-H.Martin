{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNJBsrjRmYaZ1lbfG/pdVBB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Exercises:\n","\n","#3.8 Write a program to compute unsmoothed unigrams and bigrams\n","#3.10 Add an option to your program to generate random sentences\n","#3.11 Add an option to your program to compute the perplexity of a test set"],"metadata":{"id":"BvXZfZs5CL7G"}},{"cell_type":"code","execution_count":6,"metadata":{"id":"x4P3s0ZKB-dt","executionInfo":{"status":"ok","timestamp":1753808356424,"user_tz":-420,"elapsed":9,"user":{"displayName":"Hữu Nguyễn","userId":"12104431852329722785"}}},"outputs":[],"source":["from collections import defaultdict, Counter\n","import math\n","import random\n","\n","class Unigrams():\n","  def __init__(self):\n","    self.unigrams = {}\n","    self.size = 0\n","\n","  def fit(self, sentences):\n","    #Count the frequencies\n","    for sentence in sentences:\n","      for token in sentence:\n","        if token not in self.unigrams:\n","          self.unigrams[token] = 1\n","        else:\n","          self.unigrams[token] += 1\n","    self.size = sum(self.unigrams.values())\n","    #Normalize the frequencies\n","    for token in self.unigrams.keys():\n","      self.unigrams[token] = self.unigrams[token] / self.size\n","\n","  def generate(self, size):\n","    tokens = []\n","    while len(tokens) < size:\n","      word = random.choices(list(self.unigrams.keys()), weights=list(self.unigrams.values()))[0]\n","      if word in \".!?;\":\n","        break\n","      tokens.append(word)\n","    return \" \".join(tokens)\n","\n","\n","  def perplexity(self, text):\n","    log_prob = 0\n","    epsilon = 1e-8\n","    tokens = text\n","    for token in tokens:\n","      if token in self.unigrams:\n","        log_prob += math.log(self.unigrams[token] + epsilon)\n","\n","    ppl = math.exp(-log_prob / len(tokens))\n","    return ppl\n","\n","\n","\n","class N_Grams():\n","\n","  def __init__(self, N = 2):\n","    if N < 2:\n","      raise ValueError(\"N must be greater than 1\")\n","\n","    self.N = N\n","    self.n_grams = defaultdict(lambda: defaultdict(int))\n","    self.n_minus_1_grams = defaultdict(int)\n","    self.indexMapping = {\"<s>\": 0, \"</s>\": 1}\n","    self.wordMapping = {0: \"<s>\", 1: \"</s>\"}\n","    self.seq_to_indexMapping = {\"<s>\" * (N - 1): 0}\n","    self.index_to_seqMapping = {0: \"<s>\" * (N - 1)}\n","\n","  def fit(self, sentences):\n","\n","    #Split the corpus into tokens and get the word-index mappings and seq-index mapping\n","    wordIndex = 2\n","    seqIndex = 1\n","    for sentence in sentences:\n","      tokens = [\"<s>\" for _ in range(self.N - 1)] + sentence + [\"</s>\"]\n","\n","      for i in range(self.N - 1, len(tokens)):\n","        token = tokens[i]\n","        if token not in self.indexMapping:\n","          self.indexMapping[token] = wordIndex\n","          self.wordMapping[wordIndex] = token\n","          wordIndex += 1\n","\n","        prev_seq = tokens[i - self.N + 1: i]\n","        prev_seq = \" \".join(prev_seq)\n","        if prev_seq not in self.seq_to_indexMapping:\n","          self.seq_to_indexMapping[prev_seq] = seqIndex\n","          self.index_to_seqMapping[seqIndex] = prev_seq\n","          seqIndex += 1\n","\n","    sos_index = self.indexMapping[\"<s>\"]\n","    eos_index = self.indexMapping[\"</s>\"]\n","    vocab_size = len(self.indexMapping)\n","    seq_size = len(self.seq_to_indexMapping)\n","\n","    #Get the frequencies\n","    for sentence in sentences:\n","      tokens = [\"<s>\" for _ in range(self.N - 1)] + sentence + [\"</s>\"]\n","      for i in range(self.N - 1, len(tokens)):\n","        token = tokens[i]\n","        prev_seq = tokens[i - self.N + 1: i]\n","        prev_seq = \" \".join(prev_seq)\n","\n","        token_index = self.indexMapping[token]\n","        prev_seq_index = self.seq_to_indexMapping[prev_seq]\n","\n","        self.n_grams[prev_seq_index][token_index] += 1\n","        self.n_minus_1_grams[prev_seq_index] += 1\n","\n","    #Normalize the frequencies\n","    for prev_seq in range(seq_size):\n","      for token in range(vocab_size):\n","        self.n_grams[prev_seq][token] = self.n_grams[prev_seq][token] / self.n_minus_1_grams[prev_seq]\n","\n","  def generate(self, size):\n","    prev_seq = [\"<s>\" for _ in range(self.N - 1)]\n","    tokens = []\n","\n","    while len(tokens) < size:\n","      prev_seq_index = self.seq_to_indexMapping[\" \".join(prev_seq)]\n","      available_choices = list(self.n_grams[prev_seq_index].keys())\n","      weights = list(self.n_grams[prev_seq_index].values())\n","      next_token_index = random.choices(available_choices, weights=weights, k = 1)[0]\n","      next_token = self.wordMapping[next_token_index]\n","      if next_token == \"</s>\":\n","        break\n","\n","      tokens.append(next_token)\n","      prev_seq = prev_seq[1:] + [next_token]\n","\n","    return \" \".join(tokens)\n","\n","  def perplexity(self, sentence):\n","    sentence = [\"<s>\" for _ in range(self.N - 1)] + sentence + [\"</s>\"]\n","    log_probs = 0\n","    epsilon = 1e-8\n","    for i in range(self.N - 1, len(sentence)):\n","      prev_seq = sentence[i - self.N + 1: i]\n","      prev_seq = \" \".join(prev_seq)\n","      prev_seq_index = self.seq_to_indexMapping[prev_seq]\n","      token_index = self.indexMapping[sentence[i]]\n","      log_probs += math.log(self.n_grams[prev_seq_index][token_index] + epsilon)\n","\n","    ppl = math.exp(-log_probs / len(sentence))\n","    return ppl"]},{"cell_type":"code","source":["import nltk\n","nltk.download('brown')\n","nltk.download('treebank')\n","from nltk.corpus import treebank\n","from nltk.corpus import brown\n","\n","brown_sentences = brown.sents()\n","treebank_sentences = treebank.sents()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s7suUSugEdY6","executionInfo":{"status":"ok","timestamp":1753808357072,"user_tz":-420,"elapsed":6,"user":{"displayName":"Hữu Nguyễn","userId":"12104431852329722785"}},"outputId":"81c864d1-6836-42d2-d627-534e582b0694"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Package brown is already up-to-date!\n","[nltk_data] Downloading package treebank to /root/nltk_data...\n","[nltk_data]   Package treebank is already up-to-date!\n"]}]},{"cell_type":"code","source":["#Bigrams train on Brown' corpus\n","bigrams_brown = N_Grams(2)\n","bigrams_brown.fit(brown_sentences[:1000])\n","\n","#Bigrams train on TreeBank corpus\n","bigrams_treebank = N_Grams(2)\n","bigrams_treebank.fit(treebank_sentences[:1000])\n","\n","#Unigrams train on Brown corpus\n","unigrams_brown = Unigrams()\n","unigrams_brown.fit(brown_sentences[:1000])\n","\n","#Unigrams train on treebank corpus\n","unigrams_treebank = Unigrams()\n","unigrams_treebank.fit(treebank_sentences[:1000])"],"metadata":{"id":"023mt163q7Qv","executionInfo":{"status":"ok","timestamp":1753808519418,"user_tz":-420,"elapsed":29013,"user":{"displayName":"Hữu Nguyễn","userId":"12104431852329722785"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["print(\"Bigrams trained on Brown corpus:\")\n","print(bigrams_brown.generate(20))\n","print(\"Bigrams trained on TreeBank corpus:\")\n","print(bigrams_treebank.generate(20))\n","print(\"Unigrams trained on Brown corpus:\")\n","print(unigrams_brown.generate(20))\n","print(\"Unigrams trained on TreeBank corpus:\")\n","print(unigrams_treebank.generate(20))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vq5Fz6WtIc8v","executionInfo":{"status":"ok","timestamp":1753808540922,"user_tz":-420,"elapsed":7,"user":{"displayName":"Hữu Nguyễn","userId":"12104431852329722785"}},"outputId":"24a175e7-4baf-4e01-c1fd-e4267cc6f419"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Bigrams trained on Brown corpus:\n","-- A & I didn't have been cooperating .\n","Bigrams trained on TreeBank corpus:\n","Yasser Arafat has surfaced in markets , on turnover was an early * evaluating teachers , the deal with a\n","Unigrams trained on Brown corpus:\n","obtain legislature and statement , ' the take that Club state's both one '' p.m. would the scramble there court's\n","Unigrams trained on TreeBank corpus:\n","'s Donoghue meet rate * a and dollars all refund when to to , found than `` *T*-2 have said\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"gz5BKBlVIwXJ"},"execution_count":null,"outputs":[]}]}