{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Additional exercises: Build a Multinomial naive Bayes model and binarized naive Bayes from scratch."
      ],
      "metadata": {
        "id": "EPD90geglmkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import math\n",
        "\n",
        "corpus = [[\"good\", \"good\", \"good\", \"great\", \"great\", \"great\"],\n",
        "          [\"poor\", \"great\", \"great\"],\n",
        "          [\"good\", \"poor\", \"poor\", \"poor\"],\n",
        "          [\"good\", \"poor\", \"poor\", \"poor\", \"poor\", \"poor\", \"great\", \"great\"],\n",
        "          [\"poor\", \"poor\"]]\n",
        "\n",
        "labels = [\"pos\", \"pos\", \"neg\", \"neg\", \"neg\"]\n",
        "\n",
        "test = [\"A\", \"good\", \",\", \"good\", \"plot\", \"and\", \"great\", \"characters\", \",\", \"but\", \"poor\", \"acting\"]"
      ],
      "metadata": {
        "id": "gIYUCkDNvw--"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qjC2nOAflb_v"
      },
      "outputs": [],
      "source": [
        "class MultinomialNaiveBayes:\n",
        "    def __init__(self, alpha=1.0):\n",
        "        self.alpha = alpha\n",
        "        self.vocab_size = 0\n",
        "        self.classMapping = {}\n",
        "        self.classes = []\n",
        "        self.indexMapping = {}\n",
        "        self.wordCounts = []\n",
        "        self.wordProbs = []\n",
        "        self.classCounts = []\n",
        "        self.class_priors = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "      \"\"\"\n",
        "      Fit the model according to the given training data.\n",
        "      Parameters\n",
        "      ----------\n",
        "      X : array-like of shape (n_samples, n_features)\n",
        "          Training vectors, where n_samples is the number of samples and\n",
        "          n_features is the number of features.\n",
        "      y : array-like of shape (n_samples,)\n",
        "          Target values.\n",
        "      Returns\n",
        "      -------\n",
        "      self : MultinomialNaiveBayes\n",
        "          Fitted estimator.\n",
        "      \"\"\"\n",
        "      n_samples = len(X)\n",
        "      token_index = 0\n",
        "      for i in range(n_samples):\n",
        "        #Adding new classes\n",
        "        if y[i] not in self.classes:\n",
        "          self.classMapping[y[i]] = len(self.classes)\n",
        "          self.classes.append(y[i])\n",
        "          self.wordCounts.append(defaultdict(int))\n",
        "          self.wordProbs.append(defaultdict(int))\n",
        "          self.classCounts.append(0)\n",
        "          self.class_priors.append(0)\n",
        "\n",
        "        #Get the class index and track the number of documents per class\n",
        "        class_index = self.classMapping[y[i]]\n",
        "        self.classCounts[class_index] += 1\n",
        "        sentence = X[i]\n",
        "\n",
        "        for token in sentence:\n",
        "\n",
        "          #Adding new token index\n",
        "          if token not in self.indexMapping:\n",
        "            self.indexMapping[token] = len(self.indexMapping)\n",
        "            self.vocab_size += 1\n",
        "\n",
        "          #Get the frequencies of each token\n",
        "          token_index = self.indexMapping[token]\n",
        "          self.wordCounts[class_index][token_index] += 1\n",
        "\n",
        "      for class_index in range(len(self.classes)):\n",
        "        total_count = sum(self.wordCounts[class_index].values())\n",
        "        self.class_priors[class_index] = self.classCounts[class_index] / n_samples\n",
        "\n",
        "        #Calculate the probabilities of each token in each class\n",
        "        for token_index in range(self.vocab_size):\n",
        "          word_count = self.wordCounts[class_index][token_index]\n",
        "          word_prob = (word_count + self.alpha) / (total_count + self.alpha * self.vocab_size)\n",
        "          self.wordProbs[class_index][token_index] = word_prob\n",
        "\n",
        "    def predict(self, sentence, return_probs):\n",
        "      \"\"\"\n",
        "      Perform classification on an array of test vectors X.\n",
        "      Parameters\n",
        "      ----------\n",
        "      X : array-like of shape (n_samples, n_features)\n",
        "      return_probs : return the raw probabilities of all classes (boolean)\n",
        "      Returns\n",
        "      -------\n",
        "      best_class: class that has highest probs\n",
        "      class_probs : array, shape = [n_samples]\n",
        "          Predicted target values for X.\n",
        "      \"\"\"\n",
        "      class_probs = [0] * len(self.classes)\n",
        "\n",
        "      for class_index in range(len(self.classes)):\n",
        "        arbitrary_class_prob = self.alpha / (self.classCounts[class_index] + self.alpha * self.vocab_size)\n",
        "        log_arbitrary_class_prob = math.log(arbitrary_class_prob)\n",
        "        log_sentence_prob = 0\n",
        "\n",
        "        for token in sentence:\n",
        "          if token in self.indexMapping:\n",
        "            token_index = self.indexMapping[token]\n",
        "            word_prob = self.wordProbs[class_index][token_index]\n",
        "            log_word_prob = math.log(word_prob)\n",
        "            log_sentence_prob += log_word_prob\n",
        "\n",
        "          else:\n",
        "            log_sentence_prob += log_arbitrary_class_prob\n",
        "\n",
        "        log_class_prob = math.log(self.class_priors[class_index])\n",
        "        class_probs[class_index] = math.exp(log_sentence_prob + log_class_prob)\n",
        "\n",
        "      if return_probs:\n",
        "        return class_probs\n",
        "      else:\n",
        "        best_class = self.classes[class_probs.index(max(class_probs))]\n",
        "        return best_class"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BinaryNaiveBayes:\n",
        "  def __init__(self, alpha = 1.0):\n",
        "    self.alpha = alpha\n",
        "    self.vocab_size = 0\n",
        "    self.classMapping = {}\n",
        "    self.classes = []\n",
        "    self.indexMapping = {}\n",
        "    self.wordCounts = []\n",
        "    self.wordProbs = []\n",
        "    self.classCounts = []\n",
        "    self.class_priors = []\n",
        "\n",
        "  def fit(self, X, y):\n",
        "      \"\"\"\n",
        "      Fit the model according to the given training data.\n",
        "      Parameters\n",
        "      ----------\n",
        "      X : array-like of shape (n_samples, n_features)\n",
        "          Training vectors, where n_samples is the number of samples and\n",
        "          n_features is the number of features.\n",
        "      y : array-like of shape (n_samples,)\n",
        "          Target values.\n",
        "      Returns\n",
        "      -------\n",
        "      self : MultinomialNaiveBayes\n",
        "          Fitted estimator.\n",
        "      \"\"\"\n",
        "      n_samples = len(X)\n",
        "      token_index = 0\n",
        "      for i in range(n_samples):\n",
        "\n",
        "        if y[i] not in self.classes:\n",
        "          self.classMapping[y[i]] = len(self.classes)\n",
        "          self.classes.append(y[i])\n",
        "          self.wordCounts.append(defaultdict(int))\n",
        "          self.wordProbs.append(defaultdict(int))\n",
        "          self.classCounts.append(0)\n",
        "          self.class_priors.append(0)\n",
        "\n",
        "        class_index = self.classMapping[y[i]]\n",
        "        self.classCounts[class_index] += 1\n",
        "        sentence = set(X[i])\n",
        "\n",
        "        for token in sentence:\n",
        "\n",
        "          if token not in self.indexMapping:\n",
        "            self.indexMapping[token] = len(self.indexMapping)\n",
        "            self.vocab_size += 1\n",
        "\n",
        "          token_index = self.indexMapping[token]\n",
        "          self.wordCounts[class_index][token_index] += 1\n",
        "\n",
        "      for class_index in range(len(self.classes)):\n",
        "        total_count = self.classCounts[class_index]\n",
        "        self.class_priors[class_index] = self.classCounts[class_index] / n_samples\n",
        "\n",
        "        for token_index in range(self.vocab_size):\n",
        "          word_count = self.wordCounts[class_index][token_index]\n",
        "          word_prob = (word_count + self.alpha) / (total_count + self.alpha * self.vocab_size)\n",
        "          self.wordProbs[class_index][token_index] = word_prob\n",
        "\n",
        "  def predict(self, sentence, return_probs):\n",
        "      \"\"\"\n",
        "      Perform classification on an array of test vectors X.\n",
        "      Parameters\n",
        "      ----------\n",
        "      X : array-like of shape (n_samples, n_features)\n",
        "      return_probs : return the raw probabilities of all classes (boolean)\n",
        "      Returns\n",
        "      -------\n",
        "      best_class: class that has highest probs\n",
        "      class_probs : array, shape = [n_samples]\n",
        "          Predicted target values for X.\n",
        "      \"\"\"\n",
        "      class_probs = [0] * len(self.classes)\n",
        "      sentence = set(sentence)\n",
        "\n",
        "      for class_index in range(len(self.classes)):\n",
        "        arbitrary_class_prob = self.alpha / (self.classCounts[class_index] + self.alpha * self.vocab_size)\n",
        "        log_arbitrary_class_prob = math.log(arbitrary_class_prob)\n",
        "        log_sentence_prob = 0\n",
        "\n",
        "        for token in sentence:\n",
        "          if token in self.indexMapping:\n",
        "            token_index = self.indexMapping[token]\n",
        "            word_prob = self.wordProbs[class_index][token_index]\n",
        "            log_word_prob = math.log(word_prob)\n",
        "            log_sentence_prob += log_word_prob\n",
        "\n",
        "          else:\n",
        "            log_sentence_prob += log_arbitrary_class_prob\n",
        "\n",
        "        log_class_prob = math.log(self.class_priors[class_index])\n",
        "        class_probs[class_index] = math.exp(log_sentence_prob + log_class_prob)\n",
        "\n",
        "      if return_probs:\n",
        "        return class_probs\n",
        "      else:\n",
        "        best_class = self.classes[class_probs.index(max(class_probs))]\n",
        "        return best_class"
      ],
      "metadata": {
        "id": "yR5Jog8pwP45"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.3 Use both naive Bayes models to assign a class (pos or neg) for this sentence:\n",
        "      A good, good plot and great characters, but poor acting.\n",
        "#Do the two model agree or disagree?"
      ],
      "metadata": {
        "id": "-kZVFmMf7WoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MNB_model = MultinomialNaiveBayes()\n",
        "MNB_model.fit(corpus, labels)\n",
        "\n",
        "BNB = BinaryNaiveBayes()\n",
        "BNB.fit(corpus, labels)\n",
        "\n",
        "print(MNB_model.predict(test, return_probs=True))\n",
        "print(BNB.predict(test, return_probs=True))\n",
        "\n",
        "print(\"Multinomial Naive Bayes prediction: \", MNB_model.predict(test, return_probs=False))\n",
        "print(\"Binary Naive Bayes prediction: \", BNB.predict(test, return_probs=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JssfwqXA4ZTP",
        "outputId": "04f4a9d3-1761-4715-df18-dd54479985bb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9.481481481481494e-09, 1.2702874607864575e-09]\n",
            "[4.915200000000022e-07, 2.381496723060516e-07]\n",
            "Multinomial Naive Bayes prediction:  pos\n",
            "Binary Naive Bayes prediction:  pos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EawJjytX4umt"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}